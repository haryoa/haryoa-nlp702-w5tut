{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 6: LoRA\n",
    "\n",
    "Welcome to this lab! We will talk about LoRA which has been popular lately!\n",
    "\n",
    "The objectives of this lab are as follow:\n",
    "\n",
    "1. Make you folks understand better about LoRA! Feel free to ask and discuss, we are here to learn more!\n",
    "2. You have a better understanding the underlying inner working of LoRA!\n",
    "3. Know the quote \"Don't reinvent the wheel\"? You will know how to use a  library that implement LoRA instantly where you don't have to code it FROM SCRATCH!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools\n",
    "\n",
    "Here, we use these library to implement the learning algorithm:\n",
    "1. `pytorch`\n",
    "2. `transformers`: Library to download a pre-trained model\n",
    "3. `lightning`: Tools to make training easier and no boilerplate\n",
    "4. `datasets`: load data uploaded from Huggingface\n",
    "5. `peft`: Parameter-Efficient Fine-Tuning library\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "oalalalalla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LETS GO!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import math\n",
    "from typing import Optional, List\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|██████████| 1.24G/1.24G [00:39<00:00, 31.6MB/s]\n",
      "generation_config.json: 100%|██████████| 180/180 [00:00<00:00, 66.3kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.41k/1.41k [00:00<00:00, 4.65MB/s]\n",
      "vocab.json: 100%|██████████| 2.78M/2.78M [00:01<00:00, 1.67MB/s]\n",
      "merges.txt: 100%|██████████| 1.67M/1.67M [00:00<00:00, 3.70MB/s]\n",
      "tokenizer.json: 100%|██████████| 7.03M/7.03M [00:00<00:00, 9.90MB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen1.5-0.5B-Chat\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen1.5-0.5B-Chat\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['K', 'LEN', 'APA']"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALayer():\n",
    "    def __init__(\n",
    "        self, \n",
    "        r: int, \n",
    "        lora_alpha: int, \n",
    "        lora_dropout: float,\n",
    "        merge_weights: bool,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        LoRA layer for the Qwen model.\n",
    "\n",
    "        Args:\n",
    "            r: The number of rnaks to consider for the LoRA .\n",
    "            lora_alpha: The alpha value for the LoRA layer.\n",
    "            lora_dropout: The dropout rate for the LoRA layer.\n",
    "            merge_weights: Whether to merge the weights of the LoRA layer.\n",
    "        \"\"\"\n",
    "        self.r = r\n",
    "        self.lora_alpha = lora_alpha\n",
    "        # Optional dropout\n",
    "        if lora_dropout > 0.:\n",
    "            self.lora_dropout = nn.Dropout(p=lora_dropout)\n",
    "        else:\n",
    "            self.lora_dropout = lambda x: x\n",
    "        # Mark the weight as unmerged\n",
    "        self.merged = False  # Whether the weights have been merged yet\n",
    "        self.merge_weights = merge_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLORA(nn.Module, LoRALayer):\n",
    "    \"\"\"\n",
    "    LORA for nn.Embedding\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_embeddings: int,\n",
    "        embedding_dim: int,\n",
    "        r: int = 0,\n",
    "        lora_alpha: int = 1,\n",
    "        merge_weights: bool = True,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_embeddings: Number of embeddings.\n",
    "            embedding_dim: The size of each embedding vector.\n",
    "            r: The number of ranks to consider for the LoRA.\n",
    "            lora_alpha: The alpha value for the LoRA layer.\n",
    "            merge_weights: Whether to merge the weights of the LoRA layer.\n",
    "            kwargs: Other parameters for nn.Embedding.\n",
    "        \"\"\"\n",
    "        nn.Module.__init__(self)\n",
    "        LoRALayer.__init__(self, r=r, lora_alpha=lora_alpha, lora_dropout=0,\n",
    "                           merge_weights=merge_weights)\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim, **kwargs)        \n",
    "        # Actual trainable parameters\n",
    "        if r > 0:\n",
    "            self.lora_A = nn.Parameter(self.embedding.weight.new_zeros((num_embeddings, r)))\n",
    "            self.lora_B = nn.Parameter(self.embedding.weight.new_zeros((r, embedding_dim)))\n",
    "            self.scaling = self.lora_alpha / self.r\n",
    "            # Freezing the pre-trained weight matrix (Embedding)\n",
    "            self.embedding.weight.requires_grad = False\n",
    "        self.reset_parameters()\n",
    "\n",
    "\n",
    "    def assign_object(self, obj: nn.Embedding):\n",
    "        \"\"\"\n",
    "        Assign the object to the current object.\n",
    "        Useful to copy the parameters of an existing object.\n",
    "\n",
    "        Args:\n",
    "            obj: The object to assign.\n",
    "        \"\"\"\n",
    "        self.embedding = obj\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.embedding.reset_parameters()\n",
    "        if hasattr(self, 'lora_A'):\n",
    "            # initialize A the same way as the default for nn.Linear and B to zero\n",
    "            nn.init.zeros_(self.lora_A)\n",
    "            nn.init.normal_(self.lora_B)\n",
    "\n",
    "    def train(self, mode: bool = True):\n",
    "        \"\"\"\n",
    "        From the original paper........\n",
    "        \"\"\"\n",
    "        self.embedding.train(mode)\n",
    "        if mode:\n",
    "            if self.merge_weights and self.merged:\n",
    "                # Make sure that the weights are not merged\n",
    "                if self.r > 0:\n",
    "                    self.weight.data -= (self.lora_A @ self.lora_B) * self.scaling\n",
    "                self.merged = False\n",
    "        else:\n",
    "            if self.merge_weights and not self.merged:\n",
    "                # Merge the weights and mark it\n",
    "                if self.r > 0:\n",
    "                    self.weight.data += (self.lora_A @ self.lora_B) * self.scaling\n",
    "                self.merged = True\n",
    "    \n",
    "    def merge_weights(self):\n",
    "        \"\"\"\n",
    "        Merge the weights of the LoRA layer.\n",
    "        \"\"\"\n",
    "        if self.r > 0:\n",
    "            self.embedding.weight.data += (self.lora_A @ self.lora_B) * self.scaling\n",
    "            self.merged = True\n",
    "        else:\n",
    "            raise ValueError(\"The rank parameter is not set.\")\n",
    "    \n",
    "    def unmerge_weights(self):\n",
    "        \"\"\"\n",
    "        Unmerge the weights of the LoRA layer.\n",
    "        \"\"\"\n",
    "        if self.r > 0:\n",
    "            self.embedding.data -= (self.lora_A @ self.lora_B) * self.scaling\n",
    "            self.merged = False\n",
    "        else:\n",
    "            raise ValueError(\"The rank parameter is not set.\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if self.r > 0 and not self.merged:\n",
    "            result = self.embedding.forward(x)\n",
    "            after_A = F.embedding(\n",
    "                x, self.lora_A, self.embedding.padding_idx, self.embedding.max_norm,\n",
    "                self.embedding.norm_type,  self.embedding.scale_grad_by_freq,  self.embedding.sparse\n",
    "            )\n",
    "            result += (after_A @ self.lora_B) * self.scaling\n",
    "            return result\n",
    "        else:\n",
    "            return self.embedding.forward(x)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2, 100])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed = EmbeddingLORA(50, 100, r=2)\n",
    "test_input  = torch.randint(0, 50, (4, 2))\n",
    "embed.train()\n",
    "embed(test_input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): EmbeddingLORA(151936, 1024)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2SdpaAttention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "          (up_proj): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "          (down_proj): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm()\n",
       "        (post_attention_layernorm): Qwen2RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lora(\n",
    "    model,\n",
    "    r: int,\n",
    "    lora_alpha: int,\n",
    "    merge_weights: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Recursively replaces all Embedding and Linear layers in a PyTorch model with a LORA layer.\n",
    "\n",
    "    Args:\n",
    "        model: The PyTorch model to modify.\n",
    "    \"\"\"\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, nn.Embedding):\n",
    "            print(\"Replacing\", name, \"with LORA\")\n",
    "            # Create a new instance of EmbeddingLORA with the same configurations\n",
    "            new_module = EmbeddingLORA(\n",
    "                num_embeddings=module.num_embeddings,\n",
    "                embedding_dim=module.embedding_dim,\n",
    "                padding_idx=module.padding_idx,\n",
    "                max_norm=module.max_norm,\n",
    "                norm_type=module.norm_type,\n",
    "                scale_grad_by_freq=module.scale_grad_by_freq,\n",
    "                sparse=module.sparse,\n",
    "                r=r,\n",
    "                lora_alpha=lora_alpha,\n",
    "                merge_weights=merge_weights,\n",
    "            )\n",
    "            # Copy the weights from the original embedding to the new LORA embedding\n",
    "            new_module.assign_object(module)\n",
    "            # Replace the module in the model with the new one\n",
    "            setattr(model, name, new_module)\n",
    "        else:\n",
    "            # Recursively apply the function to submodules\n",
    "            apply_lora(module, r, lora_alpha, merge_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacing embed_tokens with LORA\n"
     ]
    }
   ],
   "source": [
    "apply_lora(model, r=2, lora_alpha=1, merge_weights=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mbzuai = pd.read_csv('mbzuai.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_qwen_format(data, tokenizer):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": data['user']},\n",
    "        {\"role\": \"assistant\", \"content\": data['assistant']}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     <|im_start|>system\\nYou are a helpful assistan...\n",
       "1     <|im_start|>system\\nYou are a helpful assistan...\n",
       "2     <|im_start|>system\\nYou are a helpful assistan...\n",
       "3     <|im_start|>system\\nYou are a helpful assistan...\n",
       "4     <|im_start|>system\\nYou are a helpful assistan...\n",
       "5     <|im_start|>system\\nYou are a helpful assistan...\n",
       "6     <|im_start|>system\\nYou are a helpful assistan...\n",
       "7     <|im_start|>system\\nYou are a helpful assistan...\n",
       "8     <|im_start|>system\\nYou are a helpful assistan...\n",
       "9     <|im_start|>system\\nYou are a helpful assistan...\n",
       "10    <|im_start|>system\\nYou are a helpful assistan...\n",
       "11    <|im_start|>system\\nYou are a helpful assistan...\n",
       "12    <|im_start|>system\\nYou are a helpful assistan...\n",
       "13    <|im_start|>system\\nYou are a helpful assistan...\n",
       "14    <|im_start|>system\\nYou are a helpful assistan...\n",
       "15    <|im_start|>system\\nYou are a helpful assistan...\n",
       "16    <|im_start|>system\\nYou are a helpful assistan...\n",
       "17    <|im_start|>system\\nYou are a helpful assistan...\n",
       "18    <|im_start|>system\\nYou are a helpful assistan...\n",
       "19    <|im_start|>system\\nYou are a helpful assistan...\n",
       "20    <|im_start|>system\\nYou are a helpful assistan...\n",
       "21    <|im_start|>system\\nYou are a helpful assistan...\n",
       "22    <|im_start|>system\\nYou are a helpful assistan...\n",
       "23    <|im_start|>system\\nYou are a helpful assistan...\n",
       "24    <|im_start|>system\\nYou are a helpful assistan...\n",
       "dtype: object"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mbzuai.apply(lambda x: convert_to_qwen_format(x, tokenizer), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.Dataset.from_pandas(df_mbzuai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False,\n",
    ")\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./mbzuai\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=8,\n",
    "    save_steps=25,\n",
    "    save_total_limit=1,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_data,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sensei",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
