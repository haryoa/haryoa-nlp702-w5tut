{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22224187-86bd-4e1e-94e2-88a50644c1d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ee5d14-42e5-4cd1-a3b1-bbcc69cc5d8b",
   "metadata": {},
   "source": [
    "# Children will code lora\n",
    "1. Implementasi dulu\n",
    "2. Perbandingan finetune vs LoRA (Number of trainable params (memory) + train time) (Gauge skill mereka) Time limit, keliling\n",
    "3. See the effect plug and playnya. 2 LoRA plug and play.. LoRA MBZUAI LoRA beda task.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a1afe1-2585-4e09-b687-d4f37b5b0f9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import math\n",
    "from typing import Optional, List\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import pandas as pd\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310e32e1-05cd-40a6-90b9-4e2e9dd943a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen1.5-0.5B-Chat\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen1.5-0.5B-Chat\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ac352a-2610-4136-9251-714386d446d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LoRALayer():\n",
    "    def __init__(\n",
    "        self, \n",
    "        r: int, \n",
    "        lora_alpha: int, \n",
    "        lora_dropout: float,\n",
    "        merge_weights: bool,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        LoRA layer for the Qwen model.\n",
    "\n",
    "        Args:\n",
    "            r: The number of rnaks to consider for the LoRA .\n",
    "            lora_alpha: The alpha value for the LoRA layer.\n",
    "            lora_dropout: The dropout rate for the LoRA layer.\n",
    "            merge_weights: Whether to merge the weights of the LoRA layer.\n",
    "        \"\"\"\n",
    "        self.r = r\n",
    "        self.lora_alpha = lora_alpha\n",
    "        # Optional dropout\n",
    "        if lora_dropout > 0.:\n",
    "            self.lora_dropout = nn.Dropout(p=lora_dropout)\n",
    "        else:\n",
    "            self.lora_dropout = lambda x: x\n",
    "        # Mark the weight as unmerged\n",
    "        self.merged = False  # Whether the weights have been merged yet\n",
    "        self.merge_weights = merge_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905cda6e-5541-4b0c-9afe-81c6aa0660ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LinearLORA(nn.Module, LoRALayer):\n",
    "    \"\"\"\n",
    "    LORA for nn.Embedding\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        r: int = 0,\n",
    "        lora_alpha: int = 1,\n",
    "        lora_dropout: float = 0.,\n",
    "        fan_in_fan_out: bool = False,\n",
    "        merge_weights: bool = True,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_embeddings: Number of embeddings.\n",
    "            embedding_dim: The size of each embedding vector.\n",
    "            r: The number of ranks to consider for the LoRA.\n",
    "            lora_alpha: The alpha value for the LoRA layer.\n",
    "            merge_weights: Whether to merge the weights of the LoRA layer.\n",
    "            kwargs: Other parameters for nn.Embedding.\n",
    "        \"\"\"\n",
    "        nn.Module.__init__(self)\n",
    "        LoRALayer.__init__(self, r=r, lora_alpha=lora_alpha, lora_dropout=lora_dropout,\n",
    "                           merge_weights=merge_weights)\n",
    "        self.linear = nn.Linear(in_features, out_features, **kwargs)        \n",
    "        # Actual trainable parameters\n",
    "        if r > 0:\n",
    "            self.lora_A = nn.Parameter(self.linear.weight.new_zeros((in_features, r)))\n",
    "            self.lora_B = nn.Parameter(self.linear.weight.new_zeros((r, out_features)))\n",
    "            self.scaling = self.lora_alpha / self.r\n",
    "            # Freezing the pre-trained weight matrix (Embedding)\n",
    "            self.linear.weight.requires_grad = False\n",
    "        self.reset_parameters()\n",
    "\n",
    "\n",
    "    def assign_object(self, obj: nn.Linear):\n",
    "        \"\"\"\n",
    "        Assign the object to the current object.\n",
    "        Useful to copy the parameters of an existing object.\n",
    "\n",
    "        Args:\n",
    "            obj: The object to assign.\n",
    "        \"\"\"\n",
    "        self.linear = obj\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.linear.reset_parameters()\n",
    "        if hasattr(self, 'lora_A'):\n",
    "            # initialize A the same way as the default for nn.Linear and B to zero\n",
    "            nn.init.kaiming_uniform(self.lora_A, a=math.sqrt(5))\n",
    "            nn.init.zeros_(self.lora_B)\n",
    "\n",
    "    def train(self, mode: bool = True):\n",
    "        \"\"\"\n",
    "        From the original paper........\n",
    "        \"\"\"\n",
    "        self.linear.train(mode)\n",
    "        if mode:\n",
    "            if self.merge_weights and self.merged:\n",
    "                # Make sure that the weights are not merged\n",
    "                if self.r > 0:\n",
    "                    self.linear.weight.data -= (self.lora_A @ self.lora_B) * self.scaling\n",
    "                self.merged = False\n",
    "        else:\n",
    "            if self.merge_weights and not self.merged:\n",
    "                # Merge the weights and mark it\n",
    "                if self.r > 0:\n",
    "                    self.linear.weight.data += (self.lora_A @ self.lora_B) * self.scaling\n",
    "                self.merged = True\n",
    "    \n",
    "    def merge_weights(self):\n",
    "        \"\"\"\n",
    "        Merge the weights of the LoRA layer.\n",
    "        \"\"\"\n",
    "        if self.r > 0:\n",
    "            self.linear.weight.data += (self.lora_A @ self.lora_B) * self.scaling\n",
    "            self.merged = True\n",
    "        else:\n",
    "            raise ValueError(\"The rank parameter is not set.\")\n",
    "    \n",
    "    def unmerge_weights(self):\n",
    "        \"\"\"\n",
    "        Unmerge the weights of the LoRA layer.\n",
    "        \"\"\"\n",
    "        if self.r > 0:\n",
    "            self.linear.weight.data -= (self.lora_A @ self.lora_B) * self.scaling\n",
    "            self.merged = False\n",
    "        else:\n",
    "            raise ValueError(\"The rank parameter is not set.\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # Code this func\n",
    "        if self.r > 0 and not self.merged:\n",
    "            result = self.linear(x)\n",
    "            result += (self.lora_dropout(x) @ self.lora_A @ self.lora_B) * self.scaling\n",
    "            return result\n",
    "        else:\n",
    "            return self.linear.forward(x)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a796f3-da8a-45f8-9841-ae807125995a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EmbeddingLORA(nn.Module, LoRALayer):\n",
    "    \"\"\"\n",
    "    LORA for nn.Embedding\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_embeddings: int,\n",
    "        embedding_dim: int,\n",
    "        r: int = 0,\n",
    "        lora_alpha: int = 1,\n",
    "        merge_weights: bool = True,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_embeddings: Number of embeddings.\n",
    "            embedding_dim: The size of each embedding vector.\n",
    "            r: The number of ranks to consider for the LoRA.\n",
    "            lora_alpha: The alpha value for the LoRA layer.\n",
    "            merge_weights: Whether to merge the weights of the LoRA layer.\n",
    "            kwargs: Other parameters for nn.Embedding.\n",
    "        \"\"\"\n",
    "        nn.Module.__init__(self)\n",
    "        LoRALayer.__init__(self, r=r, lora_alpha=lora_alpha, lora_dropout=0,\n",
    "                           merge_weights=merge_weights)\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim, **kwargs)        \n",
    "        # Actual trainable parameters\n",
    "        if r > 0:\n",
    "            self.lora_A = nn.Parameter(self.embedding.weight.new_zeros((num_embeddings, r)))\n",
    "            self.lora_B = nn.Parameter(self.embedding.weight.new_zeros((r, embedding_dim)))\n",
    "            self.scaling = self.lora_alpha / self.r\n",
    "            # Freezing the pre-trained weight matrix (Embedding)\n",
    "            self.embedding.weight.requires_grad = False\n",
    "        self.reset_parameters()\n",
    "\n",
    "\n",
    "    def assign_object(self, obj: nn.Embedding):\n",
    "        \"\"\"\n",
    "        Assign the object to the current object.\n",
    "        Useful to copy the parameters of an existing object.\n",
    "\n",
    "        Args:\n",
    "            obj: The object to assign.\n",
    "        \"\"\"\n",
    "        self.embedding = obj\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.embedding.reset_parameters()\n",
    "        if hasattr(self, 'lora_A'):\n",
    "            # initialize A the same way as the default for nn.Linear and B to zero\n",
    "            nn.init.zeros_(self.lora_A)\n",
    "            nn.init.normal_(self.lora_B)\n",
    "\n",
    "    def train(self, mode: bool = True):\n",
    "        \"\"\"\n",
    "        From the original paper........\n",
    "        \"\"\"\n",
    "        self.embedding.train(mode)\n",
    "        if mode:\n",
    "            if self.merge_weights and self.merged:\n",
    "                # Make sure that the weights are not merged\n",
    "                if self.r > 0:\n",
    "                    self.embedding.weight.data -= (self.lora_A @ self.lora_B) * self.scaling\n",
    "                self.merged = False\n",
    "        else:\n",
    "            if self.merge_weights and not self.merged:\n",
    "                # Merge the weights and mark it\n",
    "                if self.r > 0:\n",
    "                    self.embedding.weight.data += (self.lora_A @ self.lora_B) * self.scaling\n",
    "                self.merged = True\n",
    "    \n",
    "    def merge_weights(self):\n",
    "        \"\"\"\n",
    "        Merge the weights of the LoRA layer.\n",
    "        \"\"\"\n",
    "        if self.r > 0:\n",
    "            self.embedding.weight.data += (self.lora_A @ self.lora_B) * self.scaling\n",
    "            self.merged = True\n",
    "        else:\n",
    "            raise ValueError(\"The rank parameter is not set.\")\n",
    "    \n",
    "    def unmerge_weights(self):\n",
    "        \"\"\"\n",
    "        Unmerge the weights of the LoRA layer.\n",
    "        \"\"\"\n",
    "        if self.r > 0:\n",
    "            self.embedding.data -= (self.lora_A @ self.lora_B) * self.scaling\n",
    "            self.merged = False\n",
    "        else:\n",
    "            raise ValueError(\"The rank parameter is not set.\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # Let them code this func\n",
    "        if self.r > 0 and not self.merged:\n",
    "            result = self.embedding.forward(x)\n",
    "            after_A = F.embedding(\n",
    "                x, self.lora_A, self.embedding.padding_idx, self.embedding.max_norm,\n",
    "                self.embedding.norm_type,  self.embedding.scale_grad_by_freq,  self.embedding.sparse\n",
    "            )\n",
    "            result += (after_A @ self.lora_B) * self.scaling\n",
    "            return result\n",
    "        else:\n",
    "            return self.embedding.forward(x)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d127c6dc-d045-4c59-b8cd-c5672af1f2db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f9f652-b07e-4a0a-b981-8ff6d2c62bc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embed = EmbeddingLORA(50, 100, r=2)\n",
    "test_input  = torch.randint(0, 50, (4, 2))\n",
    "embed.train()\n",
    "embed(test_input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cc396b-09be-40cb-a17e-a63bfafb415a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3466dc-2f60-48c6-aa8f-d0972ba7eec8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def switch_merged(\n",
    "    model,\n",
    "):\n",
    "    \"\"\"\n",
    "    Recursively replaces all Embedding and Linear layers in a PyTorch model with a LORA layer.\n",
    "\n",
    "    Args:\n",
    "        model: The PyTorch model to modify.\n",
    "    \"\"\"\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, EmbeddingLORA) or isinstance(module, LinearLORA):\n",
    "            module.merged=not module.merged\n",
    "            print(module)\n",
    "            print(module.merged, end=\" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8f4977-4002-4c01-b2ba-610e3861e240",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def apply_lora(\n",
    "    model,\n",
    "    r: int,\n",
    "    lora_alpha: int,\n",
    "    merge_weights: bool = False,\n",
    "    lora_dropout: float = 0.0,\n",
    "    replace_embedding: bool = True,\n",
    "    replace_linear: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Recursively replaces all Embedding and Linear layers in a PyTorch model with a LORA layer.\n",
    "\n",
    "    Args:\n",
    "        model: The PyTorch model to modify.\n",
    "    \"\"\"\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, EmbeddingLORA) or isinstance(module, LinearLORA):\n",
    "            continue\n",
    "        if isinstance(module, nn.Embedding) and replace_embedding:\n",
    "            # Create a new instance of EmbeddingLORA with the same configurations\n",
    "            new_module = EmbeddingLORA(\n",
    "                num_embeddings=module.num_embeddings,\n",
    "                embedding_dim=module.embedding_dim,\n",
    "                padding_idx=module.padding_idx,\n",
    "                max_norm=module.max_norm,\n",
    "                norm_type=module.norm_type,\n",
    "                scale_grad_by_freq=module.scale_grad_by_freq,\n",
    "                sparse=module.sparse,\n",
    "                r=r,\n",
    "                lora_alpha=lora_alpha,\n",
    "                merge_weights=merge_weights,\n",
    "            )\n",
    "            # Copy the weights from the original embedding to the new LORA embedding\n",
    "            new_module.assign_object(module)\n",
    "            # Replace the module in the model with the new one\n",
    "            setattr(model, name, new_module)\n",
    "        elif isinstance(module, nn.Linear) and replace_linear:\n",
    "            new_module = LinearLORA(\n",
    "                in_features = module.in_features,\n",
    "                out_features= module.out_features,\n",
    "                r = r,\n",
    "                lora_alpha = lora_alpha,\n",
    "                lora_dropout = lora_dropout,\n",
    "                fan_in_fan_out = False,\n",
    "                merge_weights = merge_weights,\n",
    "            )\n",
    "            new_module.assign_object(module)\n",
    "            setattr(model, name, new_module)\n",
    "        else:\n",
    "            # Recursively apply the function to submodules\n",
    "            apply_lora(module, r, lora_alpha, merge_weights, lora_dropout=lora_dropout, replace_embedding=replace_embedding, replace_linear=replace_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbaaee5-3eee-490f-af52-d90a64292163",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "apply_lora(model, r=2, lora_alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1538082d-c534-46e7-ba0b-59034c4e9057",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9356355a-7ed1-4a47-89c4-f25c337da317",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_mbzuai = pd.read_csv('mbzuai.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e005bf9-46f8-4d35-9968-5f9a90824a67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_to_qwen_format(data, tokenizer):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": data['user']},\n",
    "        {\"role\": \"assistant\", \"content\": data['assistant']}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78283124-e93c-4faa-8a9f-dddcf580c89b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_mbzuai['text'] = df_mbzuai.apply(lambda x: convert_to_qwen_format(x, tokenizer), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1fd1bc-fcc2-4c7d-9505-55631a329a16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_mbzuai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadfce6d-5aba-4eaf-abd5-bf902e3f11a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = datasets.Dataset.from_pandas(df_mbzuai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4943f3-635e-448b-9427-aa654a1696e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(x, tokenizer):\n",
    "    return tokenizer(x['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1584c8c1-4f50-4ed8-8fe0-01a9a349fed8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_mbzuai.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11180b5b-6ba2-4eea-9414-6c6916114924",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = train_data.map(lambda x: tokenize(x, tokenizer), batched=True, batch_size=16, remove_columns=df_mbzuai.columns.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63704654-f97b-4875-ac13-0871bb70dbc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if \"lora\" not in name:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d11a89-95c6-4922-8d31-6341120c60d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b521fc-5225-48f0-adda-8cf688e14e5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False,\n",
    ")\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./mbzuai\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs= 50,\n",
    "    per_device_train_batch_size=16,\n",
    "    save_total_limit=1,\n",
    "    learning_rate=1e-4\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97965368-8ccf-4ca0-8c1a-f818205af45f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_answer(model, tokenizer, prompt):\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        model_inputs.input_ids,\n",
    "        max_new_tokens=512\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7180d092-a835-42c1-856b-b9b7b4a2a6e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generate_answer(model, tokenizer, \"Who is Prabowo Subianto?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0002e7d-828f-4283-a995-80b527911237",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generate_answer(model, tokenizer, \"Which one is the best? MBZUAI or Universitas Indonesia? \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8190e17a-ca58-46dc-9f29-3e3df802d5af",
   "metadata": {},
   "source": [
    "## Set off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3762ba4a-7d75-4e94-a6d6-ed5b6209148d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def switch_merged(\n",
    "    model,\n",
    "):\n",
    "    \"\"\"\n",
    "    Recursively replaces all Embedding and Linear layers in a PyTorch model with a LORA layer.\n",
    "\n",
    "    Args:\n",
    "        model: The PyTorch model to modify.\n",
    "    \"\"\"\n",
    "    for name, module in model.named_children():\n",
    "        switch_merged(module)\n",
    "        if isinstance(module, EmbeddingLORA) or isinstance(module, LinearLORA):\n",
    "            module.merged=not module.merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d64edb-4344-4359-8b03-b8aeba77a650",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Without LORA\")\n",
    "print(\"-------\")\n",
    "switch_merged(model)\n",
    "print(generate_answer(model, tokenizer, \"How to register to MBZUAI?\"))\n",
    "print(generate_answer(model, tokenizer, \"Write a poem related to MBZUAI!\"))\n",
    "print(\"-------\")\n",
    "\n",
    "print(\"With LORA\")\n",
    "print(\"-------\")\n",
    "\n",
    "switch_merged(model)\n",
    "print(generate_answer(model, tokenizer, \"How to register to MBZUAI?\"))\n",
    "print(generate_answer(model, tokenizer, \"Write a poem related to MBZUAI!\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7be2686-3a2c-48bf-bda4-db1b81b0b7f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(generate_answer(model, tokenizer, \"Where is MBZUAI located?\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cee236-df09-4ba4-bfba-d16e75775861",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "1. Tidy the notebook\n",
    "2. Add save LORA weight function\n",
    "3. Add attach LORA weight function\n",
    "4. Add remove LORA weight function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5583ac81-84e6-44e5-b200-418d4b378268",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.g5.xlarge",
  "kernelspec": {
   "display_name": "nlp702 (arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-2.1.0-gpu-py310)",
   "language": "python",
   "name": "myenv__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-2.1.0-gpu-py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
